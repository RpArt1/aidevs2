{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector store example \n",
    "from https://bravecourses.circle.so/c/lekcje-programu/c03l03-wyszukiwanie-i-bazy-przetwarzanie-dlugich-dokumentow \n",
    "& https://github.com/i-am-alice/2nd-devs/blob/main/21_similarity/helpers.ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Search \n",
    "\n",
    "## Step 1 \n",
    "- load data & \n",
    "- chunk it \n",
    "- create store \n",
    "or load previously created store \n",
    "\n",
    "this is exmample of how load simple text document to Chroma db and save it on dist. \n",
    "Then you can retreive it or create new db \n",
    "\n",
    "#### what is OpenAIEmbeddings class ? \n",
    "wrapper around OpenAI Embeddings API \n",
    "purpose:  OpenAIEmbeddings class in LangChain uses OpenAI's API to generate embeddings for the input text\n",
    "https://github.com/langchain-ai/langchain/issues/12314\n",
    "\n",
    "- first encodes the texts into tokens using the tiktoken package\n",
    "- tokens are then split into chunks of a maximum length specified by the embedding_ctx_length attribute\n",
    "- chunks are sent to the OpenAI API in batches of a size specified by the chunk_size attribute\n",
    "- API response is then processed to extract the embeddings\n",
    "    - If the skip_empty attribute is set to True, any empty embeddings are skipped\n",
    "    - Otherwise an error is thrown\n",
    "- Finally, the method averages the embeddings for each text, normalizes them, and returns them as a list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "import dotenv\n",
    "import codecs\n",
    "from os import environ\n",
    "\n",
    "# prep openapi key\n",
    "env_file = './.env'\n",
    "dotenv.load_dotenv(env_file, override=True)\n",
    "print('MY_VAR = ', environ.get('MY_VAR'))\n",
    "embedding_function=OpenAIEmbeddings(api_key=environ.get('OPENAI_API_KEY'))\n",
    "\n",
    "\n",
    "def prepare_documents(file_path):\n",
    "    if file_path.endswith(\".txt\"):  # assuming the documents are text files\n",
    "        with codecs.open(file_path, 'r', encoding='utf8') as f:\n",
    "            raw_document = f.read()\n",
    "            return raw_document.split(\"\\n\\n\")\n",
    "\n",
    "def get_vector_store(load_new_chroma: bool=True) -> Chroma:\n",
    "\n",
    "    existing_db = Chroma(persist_directory='various/vector_store_db', embedding_function=OpenAIEmbeddings())\n",
    "\n",
    "    if not load_new_chroma and len(existing_db.get()) > 0:\n",
    "        db = existing_db\n",
    "        print('Loaded existing vector store')\n",
    "    else: \n",
    "        \n",
    "        ## COMENTED DOESN\"T WORK AND ALL ANY SPLITTER LANGCHAIND DOESN\"T WORK \n",
    "        ## USE prepare_documents() instead !!!!\n",
    "\n",
    "        # loader = TextLoader('various/vector-store-example.txt')\n",
    "        # documents = loader.load()\n",
    "        # text_splitter = RecursiveCharacterTextSplitter( chunk_size=10000,\n",
    "        #     chunk_overlap=100,\n",
    "        #     separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"])\n",
    "        # docs = text_splitter.split_documents(documents) ## returns  -> List[Document]:\n",
    "      \n",
    "        documents = prepare_documents('various/vector-store-example.txt')\n",
    "        db = Chroma.from_texts(documents, OpenAIEmbeddings(), persist_directory='various/vector_store_db')\n",
    "        print('Created new vector store')\n",
    "    return db\n",
    "\n",
    "\n",
    "#db = get_vector_store(True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### DOENST WOKR  ########\n",
    "\n",
    "### ANY ATEMPT  TO WORK WITH LANGCHAIN SPLITTER FAILS DON'T USE \n",
    "\n",
    "## leaving it here to remember that it doesn't work :)\n",
    "\n",
    "\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# r_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,\n",
    "#     chunk_overlap=0,separators=[\"\\n\"])\n",
    "\n",
    "# test = \"\"\"a\\nbcefg\\nhij\\nk\"\"\"\n",
    "# print(len(test))\n",
    "# tmp = r_splitter.split_text(test)\n",
    "# print(tmp)\n",
    "\n",
    "\n",
    "### that doesn't work its only spliting document when chunk_size is small like 100 it doesn't use separators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 Now we need to search Chroma db with some queries\n",
    "\n",
    "- similarity_search takes topK which defines how many elements are to be returned \n",
    "- it can be helpfull to return more and then filter /group them by f.e tags \n",
    "- filtering can be done right on search because most vecor db allows us to pass object that defines metadata similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Do you know the name of Adams dog?'\n",
    "vector_store = get_vector_store(False)\n",
    "filters = {\"page_title\": \"Adam\"}\n",
    "found_docs = vector_store.similarity_search_with_score(query, topK=0, filters=None)\n",
    "print(found_docs)\n",
    "#print(f\"This will be my context for AI {found_docs[0].page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another example with loading documents from code not ext file \n",
    "\n",
    "#### something i don;t get about chroma \n",
    "is that is seems to persist its objects even after restart \n",
    "and when new run is done documents are doubled ...  \n",
    "\n",
    "this is simple example wehere data is in documents and data is not partitioned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import dotenv\n",
    "from os import environ\n",
    "# prep openapi key\n",
    "env_file = './.env'\n",
    "dotenv.load_dotenv(env_file, override=True)\n",
    "doc=Document(\n",
    "                page_content=\"text\",\n",
    "                metadata={\"source\": \"local\"}\n",
    "            )\n",
    "print(f\" this is doc:  {doc.page_content}\")\n",
    "\n",
    "my_personal_docs = [\n",
    "    Document(page_content=\"Charles is a xmen.\"),\n",
    "    Document(page_content=\"Charles has a Cerebro czy jakos tak.\"),\n",
    "    Document(page_content=\"Charles is also a proper wheeler.\") \n",
    "]\n",
    "\n",
    "vector_store_3 = Chroma.from_documents(my_personal_docs, embedding=OpenAIEmbeddings(api_key=environ.get('OPENAI_API_KEY')),persist_directory=False)\n",
    "\n",
    "result  = vector_store_3.similarity_search(\"What does Charles do?\",2);\n",
    "\n",
    "print(f\"vector store result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this is example where data is partitioned\n",
    "\n",
    "- 2nd chun of data is not returned despite extending result count to 3 \n",
    "- that needs to be adressed ( to be done in next lessons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this is doc:  text\n",
      "vector store result: [Document(page_content='Charles is a mutant who specializes in mind controll'), Document(page_content='Charles is also a proper wheeler.'), Document(page_content='Charles has an nice spaceship.')]\n"
     ]
    }
   ],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from os import environ\n",
    "\n",
    "my_personal_docs = [\n",
    "    Document(page_content=\"Charles is a mutant who specializes in mind controll\"),\n",
    "    Document(page_content=\"with a particular focus on blowing those minds.\"),\n",
    "    Document(page_content=\"Charles has an nice spaceship.\"),\n",
    "    Document(page_content=\"Charles is also a proper wheeler.\") \n",
    "\n",
    "]\n",
    "\n",
    "vector_store_3 = Chroma.from_documents(my_personal_docs, embedding=OpenAIEmbeddings(api_key=environ.get('OPENAI_API_KEY')),persist_directory=False)\n",
    "\n",
    "result  = vector_store_3.similarity_search(\"What does Charles do?\", 3);\n",
    "\n",
    "print(f\"vector store result: {result}\")\n",
    "# ector store result: [\n",
    "    # Document(page_content='Charles is a mutant who specializes in mind controll'), \n",
    "    # Document(page_content='Charles is also a proper wheeler.'), \n",
    "    # Document(page_content='Charles has an nice spaceship.')]\n",
    "# missing the one with blowing minds which should be part of first response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wyszukiwanie hybrydowe\n",
    "\n",
    "- HSRAG Hybrid Search and Retrieval Augmented Generation.\n",
    "    - this joins normal db searching with vector dbsearching \n",
    "- \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
